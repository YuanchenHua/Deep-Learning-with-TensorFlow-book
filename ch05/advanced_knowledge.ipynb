{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def printl(a):\n",
    "    print(a)\n",
    "    print('===============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并与分割\n",
    "### `tf.concat(tensors, axis)`\n",
    "合并操作可以在任意的维度上进行，唯一的约束是**非合并维度的长度必须一致**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([5,10,2],60,20)\n",
    "b = tf.random.normal([4,10,2],60,20)\n",
    "c = tf.concat([a,b],0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.stack(tensors, axis)`\n",
    "可以合并多个张量 tensors，其中 axis 指定插入新维度的位 置，axis 的用法与 tf.expand_dims 的一致  \n",
    "一般将较大概念放在前面  \n",
    "tf.stack 也需要满足张量堆叠合并条件，它需要所有合并的张量 **shape 完全一致**才可合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([10,2],60,20)\n",
    "b = tf.random.normal([10,2],60,20)\n",
    "c = tf.random.normal([10,2],60,20)\n",
    "d = tf.stack([a,b,c],axis = 0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.split(x, num_or_size_splits, axis)`\n",
    "num_or_size_splits：切割方案。当 num_or_size_splits 为单个数值时，如 10，表示切割为 10 份；当 num_or_size_splits 为 List 时，每个元素表示每份的长度，如$[2,4,2,2]$表示切割为 4 份，每份的长度分别为 2,4,2,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=219, shape=(4, 4), dtype=float32, numpy=\n",
       " array([[ 8.324749 , 18.093138 ,  4.654476 , 10.848534 ],\n",
       "        [17.785992 ,  8.864199 ,  5.37084  ,  8.481268 ],\n",
       "        [11.68577  ,  6.0778313, 11.092072 ,  4.6486797],\n",
       "        [19.951597 , 10.581699 ,  9.458782 ,  5.152624 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=220, shape=(4, 2), dtype=float32, numpy=\n",
       " array([[14.483393 ,  6.524456 ],\n",
       "        [14.980684 , 15.158613 ],\n",
       "        [12.288662 , 14.867704 ],\n",
       "        [11.434956 ,  8.6031685]], dtype=float32)>,\n",
       " <tf.Tensor: id=221, shape=(4, 4), dtype=float32, numpy=\n",
       " array([[18.274652 ,  8.104332 , 17.56971  ,  8.417494 ],\n",
       "        [14.602753 , 14.266607 , 17.57175  ,  5.9118385],\n",
       "        [ 8.782787 ,  6.4340477,  6.114914 , 15.49366  ],\n",
       "        [ 6.2360725,  6.7435417, 13.426455 , 17.905296 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.uniform([4,10],4,20)\n",
    "# tf.split(a,axis = 1,num_or_size_splits =[4,2,4])\n",
    "tf.split(a,[4,2,4],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.unstack(x, axis=0)`\n",
    "```python\n",
    "tf.unstack(\n",
    "    value,\n",
    "    num=None,\n",
    "    axis=0,\n",
    "    name='unstack'\n",
    ")\n",
    "```\n",
    "如果希望在某个维度上全部按长度为 1 的方式分割,这种方式是 tf.split 的一种特殊情况，切割长度固定为 1，只需要指定切割维度即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=259, shape=(5, 2), dtype=float32, numpy=\n",
       " array([[0.50755024, 0.6554961 ],\n",
       "        [0.4694265 , 0.9796933 ],\n",
       "        [0.3970158 , 0.3138553 ],\n",
       "        [0.9163903 , 0.20893252],\n",
       "        [0.38917017, 0.2810353 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=260, shape=(5, 2), dtype=float32, numpy=\n",
       " array([[0.36020386, 0.9185802 ],\n",
       "        [0.39497042, 0.7420286 ],\n",
       "        [0.5970417 , 0.42126787],\n",
       "        [0.78616714, 0.8457265 ],\n",
       "        [0.37590563, 0.9375638 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.uniform([2,5,2])\n",
    "tf.unstack(a,axis = 0) # shape 中的0纬没了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据统计\n",
    "## norm\n",
    "1. L1 范数，定义为向量𝒙的所有元素绝对值之和\n",
    "2. L2 范数，定义为向量𝒙的所有元素的平方和，再开根号\n",
    "3. ∞ −范数，定义为向量𝒙的所有元素绝对值的最大值\n",
    "4. 对于矩阵、张量，同样可以利用向量范数的计算公式，等价于**将矩阵、张量打平**成向量后计算\n",
    "### `tf.norm(x, ord)`\n",
    "参数 ord 指定为 1,2 时计算 L1, L2 范数，指定为 np.inf 时计算∞ −范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "===============================================\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "===============================================\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([2,2])\n",
    "printl(tf.norm(a,ord = 1))\n",
    "printl(tf.norm(a,ord = 2))\n",
    "printl(tf.norm(a,ord = np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.reduce_max(a,axis)`, `tf.reduce_min(a,axis)`, `tf.reduce_mean(a,axis)`, `tf.reduce_sum(a,axis)`\n",
    "可以指定axis，如果不指定，默认求解全局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=282, shape=(3,), dtype=int32, numpy=array([2, 3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,2,3],[2,3,4]])\n",
    "tf.reduce_max(a,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在求解误差函数时，通过 TensorFlow 的 MSE 误差函数可以求得每个样本的误差，需要计算样本的平均误差，此时可以通过 tf.reduce_mean 在样本数维度上计算均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=309, shape=(), dtype=float32, numpy=0.74678636>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tf.random.normal([4,10]) # 网络预测输出\n",
    "y = tf.constant([1,2,2,0]) # 真实标签\n",
    "y = tf.one_hot(y,depth=10) # one-hot 编码\n",
    "loss = tf.keras.losses.mse(y,out) # 计算每个样本的误差\n",
    "loss = tf.reduce_mean(loss) # 平均误差\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.nn.softmax(a, axis)`\n",
    "分类任务的标签 预测,由于元素的位置索引代表了当前样本属于此类别的概率，预测时往往会选择概率值最大的元素所在的索引号作为样本类别的预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.14587018  0.49652064  0.7904653   0.448307    0.78517324  1.0040271\n",
      "   0.44834936 -0.2258144  -1.244064    0.3923904 ]\n",
      " [-0.6205086  -0.01196219 -0.2933465   1.3244168   0.68107426  1.7891818\n",
      "   0.3372692  -0.90958375 -1.3580241  -0.9254841 ]], shape=(2, 10), dtype=float32)\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=344, shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.05637143, 0.10716324, 0.14378196, 0.10211908, 0.14302306,\n",
       "        0.17801356, 0.10212341, 0.05204029, 0.01879834, 0.09656564],\n",
       "       [0.03268635, 0.0600696 , 0.04533682, 0.22857921, 0.12012589,\n",
       "        0.36381587, 0.08517732, 0.02448063, 0.01563391, 0.02409446]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([2,10])\n",
    "printl(a)\n",
    "a = tf.nn.softmax(a, axis=1) # 通过 softmax 转换为概率值\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求索引\n",
    "### `tf.argmax(x, axis)`，`tf.argmin(x, axis)`\n",
    "可以求解在 axis 轴上，x 的最大值 在的索引号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=348, shape=(2,), dtype=int64, numpy=array([5, 5])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(a,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量比较\n",
    "### 没math也行，参数为纬度一样的张量，好像直接用运算符也一样哦\n",
    "`tf.math.greater(tensor,tensor)`, `tf.math.less()`, `tf.math.greater_equal()`,`tf.math.less_equal()`, `tf.math.not_equal()`, `tf.math.is_nan()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.53, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "out = tf.random.normal([100,10])\n",
    "out = tf.nn.softmax(out, axis=1) # 输出转换为概率\n",
    "pred = tf.argmax(out, axis=1) # 选取预测值\n",
    "y = tf.random.uniform([100],dtype=tf.int64,maxval=10) # 真实标签\n",
    "out = tf.less_equal(pred,y)\n",
    "out = tf.cast(out,dtype = tf.float16) # 布尔型转 int 型\n",
    "correct = tf.reduce_sum(out) # 统计 True 的个数\n",
    "print(correct/out.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True False False False False]\n",
      " [False  True  True  True  True]], shape=(2, 5), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "a = tf.range(10)\n",
    "a = tf.reshape(a,(2,5))\n",
    "b =tf.constant([[0,3,4,6,8],[10,3,5,7,9]])\n",
    "c = a>=b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 填充与复制\n",
    "之前我们介绍了通过复制的方式可以增加数据的长度，但是重复复制数据会破坏原有的数据结构，并不适合于此处。通常的做法是，在需要补充长度的信号开始或结束处填充足够数量的特定数值，如 0，使得填充后 的长度满足系统要求。那么这种操作就叫做填充(Padding)。\n",
    "### `tf.pad(x, paddings)`\n",
    "paddings 是包含了多个 $[𝐿𝑒𝑓𝑡 𝑃𝑎𝑑𝑑𝑖𝑛𝑔, 𝑅𝑖𝑔ℎ𝑡 𝑃𝑎𝑑𝑑𝑖𝑛𝑔]$的嵌套方案 List，如$[[0,0],[2,1],[1,2]]$表示第一个维度不填充，第二个维度左边(起始处)填充两个单元，右边(结束处)填充一个单元，第三个维度左边 填充一个单元，右边填充两个单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=852, shape=(6,), dtype=int32, numpy=array([7, 8, 1, 6, 0, 0], dtype=int32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1,2,3,4,5,6])\n",
    "b = tf.constant([7,8,1,6])\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "# 需要在第一个维度的右边，填充2个0\n",
    "b = tf.pad(b, [[0,2]]) # 填充\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=853, shape=(2, 6), dtype=int32, numpy=\n",
       "array([[1, 2, 3, 4, 5, 6],\n",
       "       [7, 8, 1, 6, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([a,b],axis=0) # 合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理中，需要加载不同句子长度的数据集，有些句子长度较小，如 10 个单词左右，部份句子长度较长，如超过 100 个单词。为了能够保存在同一张量中，一般会选取能够覆盖大部分句子长度的阈值，如 80 个单词：对于小于 80 个单词的句子，在末尾填充相应数量的 0；对大于 80 个单词的句子，截断超过规定长度的部分单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 80) (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "total_words = 10000 # 设定词汇量大小\n",
    "max_review_len = 80 # 最大句子长度\n",
    "embedding_len = 100 # 词向量长度\n",
    "# 加载 IMDB 数据集\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=total_words)\n",
    "\n",
    "# 将句子填充或截断到相同长度，设置为末尾填充和末尾截断方式\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len,truncating='post',padding='post')\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len,truncating='post',padding='post')\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据限幅\n",
    "### `tf.maximum(x, a)`,`tf.minimum(x, a)`\n",
    "可以通过 tf.maximum(x, a)实现数据的下限幅：$𝑥 ∈ [𝑎, +∞)$；可以 通过 tf.minimum(x, a)实现数据的上限幅：$𝑥 ∈ (−∞,𝑎]$， \n",
    "通过组合 tf.maximum(x, a)和 tf.minimum(x, b)可以实现同时对数据的上下边界限幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 3 3 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.range(10)\n",
    "x = tf.maximum(x,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=861, shape=(10,), dtype=int32, numpy=array([3, 3, 3, 3, 4, 5, 6, 7, 8, 8], dtype=int32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.minimum(x,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.clip_by_value()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=869, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 7], dtype=int32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(9)\n",
    "tf.clip_by_value(x,2,7) # 限幅为 2~7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高级操作\n",
    "### `tf.gather(data,List(index),axis)`\n",
    "可以实现根据索引号收集数据的目的  \n",
    "实际上，对于上述需求，通过切片$𝑥[: 2]$可以更加方便地实现。但是对于不规则的索引方式，比如，第 1,4,9,12,13,27，则切片方式实现起来非常麻烦，而 tf.gather 则是针对于此需求设计的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32)\n",
    "x = tf.gather(x,[0,1],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中索引号可以乱序排列，此时收集的数据也是对应顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=885, shape=(4, 2), dtype=int32, numpy=\n",
       "array([[6, 7],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.range(8)\n",
    "a=tf.reshape(a,[4,2]) # 生成张量 a\n",
    "tf.gather(a,[3,1,0,2],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.gather_nd(List)`\n",
    "通过指定每次采样的坐标来实现采样多个点的目的  \n",
    "将坐标合并为一个 List 参数  \n",
    "最大List中，每一个元素是采样的坐标，每一个元素从左到右分别是原数据中的纬度  \n",
    "可重复采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=893, shape=(3, 4), dtype=int32, numpy=\n",
       "array([[4, 5, 6, 7],\n",
       "       [4, 5, 6, 7],\n",
       "       [0, 1, 2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.range(8)\n",
    "a = tf.reshape(a,(2,4))\n",
    "tf.gather_nd(a,([1],[1],[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=895, shape=(3,), dtype=int32, numpy=array([5, 7, 2], dtype=int32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(a,([1,1],[1,3],[0,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.boolean_mask(x, mask, axis)`\n",
    "`𝑚𝑎𝑠𝑘 = [𝑇𝑟𝑢𝑒, 𝐹𝑎𝑙𝑠𝑒, 𝐹𝑎𝑙𝑠𝑒, 𝑇𝑟𝑢𝑒]`  \n",
    "注意掩码的长度必须与对应维度的长度一致,`tf.boolean_mask` 的用法其实与 `tf.gather` 非常类似，只不过一个通过掩码方式采样，一个直接给出索引号采样  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=929, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.range(8)\n",
    "a = tf.reshape(a,(2,4))\n",
    "mask = [True,True,False,False]\n",
    "tf.boolean_mask(a,mask,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以多掩码采样，多掩码采样的时候，不用指定axis  \n",
    "纬度默认左对齐，也就是和`tf.gather_nd`一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=963, shape=(4,), dtype=int32, numpy=array([0, 1, 4, 5], dtype=int32)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.range(8)\n",
    "a = tf.reshape(a,(2,4))\n",
    "mask = [[True,True,False,False],[True,True,False,False]]\n",
    "tf.boolean_mask(a,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.where(cond, a, b)`\n",
    "根据 cond 条件的真假从 a 或 b 中读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=971, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.ones([3,3]) # 构造 a 为全 1\n",
    "b = tf.zeros([3,3]) # 构造 b 为全 0\n",
    "cond =tf.constant([[True,False,False],[False,True,False],[True,True,False]])\n",
    "tf.where(cond,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 a=b=None 即 a,b 参数不指定时，tf.where 会返回 cond 张量中所有 True 的元素的索引坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=973, shape=(4, 2), dtype=int64, numpy=\n",
       "array([[0, 0],\n",
       "       [1, 1],\n",
       "       [2, 0],\n",
       "       [2, 1]])>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond =tf.constant([[True,False,False],[False,True,False],[True,True,False]])\n",
    "tf.where(cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法：我们需要提取张量中所有正数的数据和索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=983, shape=(8,), dtype=float32, numpy=\n",
       "array([0.21453662, 0.9720028 , 1.2281733 , 0.95435435, 0.6219342 ,\n",
       "       0.50699323, 0.6760889 , 1.3397388 ], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([3,3]) # 构造 a\n",
    "mask = x>0 # 比较操作，等同于 tf.equal() 通过比较运算，得到正数的掩码\n",
    "# 通过 tf.where 提取此掩码处 True 元素的索引 \n",
    "indices=tf.where(mask)\n",
    "# 拿到索引后，通过 tf.gather_nd 即可恢复出所有正数的元素\n",
    "tf.gather_nd(x,indices)\n",
    "# 实际上，当我们得到掩码 mask 之后，也可以直接通过 tf.boolean_mask 获取对于元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scatter_nd(indices, updates, shape)`\n",
    "可以高效地刷新张量的部分数据，但是只能在 全 0 张量的白板上面刷新，因此可能需要结合其他操作来实现现有张量的数据刷新功能  \n",
    "构造位置 indices 和`gather_nd`函数一样，左对齐指定，可以指定很多纬。指定的纬度越多，构造插入的数据的纬度就越少，两者加起来是总纬度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1075, shape=(4, 4, 4), dtype=int32, numpy=\n",
       "array([[[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[5, 5, 5, 5],\n",
       "        [6, 6, 6, 6],\n",
       "        [7, 7, 7, 7],\n",
       "        [8, 8, 8, 8]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3],\n",
       "        [4, 4, 4, 4]]], dtype=int32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造写入位置\n",
    "indices = tf.constant([[1],[3]])\n",
    "updates = tf.constant([# 构造写入数据\n",
    "[[5,5,5,5],[6,6,6,6],[7,7,7,7],[8,8,8,8]],\n",
    "[[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]]\n",
    "])\n",
    "print(updates.shape)\n",
    "# 在 shape 为[4,4,4]白板上根据 indices 写入 updates\n",
    "tf.scatter_nd(indices,updates,[4,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1079, shape=(4, 4, 4), dtype=int32, numpy=\n",
       "array([[[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [5, 5, 5, 5],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [0, 0, 0, 0]]], dtype=int32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造写入位置\n",
    "indices = tf.constant([[1,2],[3,2]])\n",
    "updates = tf.constant([# 构造写入数据\n",
    "[5,5,5,5],\n",
    "[1,1,1,1]])\n",
    "print(updates.shape)\n",
    "# 在 shape 为[4,4,4]白板上根据 indices 写入 updates\n",
    "tf.scatter_nd(indices,updates,[4,4,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.meshgrid()`\n",
    "方便地生成二维网格采样点坐标，方便可视化等应用场合  \n",
    "通过在 x 轴上进行采样 100 个数据点，y 轴上采样 100 个数据点，然后通过 tf.meshgrid(x, y)即可返回这 10000 个数据点的张量数据，shape 为$[100,100,2]$。为了方便计 算，tf.meshgrid 会返回在 axis=2 维度切割后的 2 个张量 a,b，其中张量 a 包含了所有点的 x 坐标，b 包含了所有点的 y 坐标，shape 都为$[100,100]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([100, 100]), TensorShape([100, 100]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.linspace(-8.,8,100) # 设置 x 坐标的间隔\n",
    "y = tf.linspace(-8.,8,100) # 设置 y 坐标的间隔\n",
    "x,y = tf.meshgrid(x,y) # 生成网格点，并拆分后返回\n",
    "x.shape,y.shape # 打印拆分后的所有点的 x,y 坐标张量 shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经典数据集加载\n",
    "1. 通过 `datasets.xxx.load_data()`即可实现经典数据集的自动加载，其中 xxx 代表具体的数 据集名称。  \n",
    "2. 会返回 2 个 tuple，第一个 tuple 保存了用于训练的数据 x,y 训练集对象；第 2 个 tuple 则保存了用于测试的数据 x_test,y_test 测试集对象，所有的数据都用 Numpy.array 容器承载  \n",
    "3. 数据加载进入内存后，需要转换成 Dataset 对象，以利用 TensorFlow 提供的各种便捷功能。通过 `Dataset.from_tensor_slices` 可以将训练部分的数据图片 x 和标签 y 都转换成 Dataset 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据转换成 Dataset 对象后，一般需要再添加一系列的数据集标准处理步骤，如随机打 散，预处理，按批装载等。\n",
    "## 随机打散\n",
    "### `Dataset.shuffle(buffer_size)`\n",
    "防止每次训练时数据按固定顺序产生，从而使得模型尝试“记忆”住标签信息  \n",
    "其中 buffer_size 指定缓冲池的大小，一般设置为一个较大的参数即可。通过 Dataset 提供的这些工具函数会返回新的 Dataset 对象，可以通过\n",
    "\n",
    "`db = db.shuffle().step2().step3.()`\n",
    "\n",
    "方式完成所有的数据处理步骤，实现起来非常方便。\n",
    "## 批训练\n",
    "`train_db = train_db.batch(128)`  \n",
    "\n",
    "其中 128 为 batch size 参数，即一次并行计算 128 个样本的数据。Batch size 一般根据用户的 GPU 显存资源来设置，当显存不足时，可以适量减少 batch size 来减少算法的显存使用量。\n",
    "## 预处理\n",
    "预处理函数实现在 preprocess 函数中，传入函数引用即可。preprocess就是实现自己预处理的函数\n",
    "\n",
    "`train_db = train_db.map(preprocess)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y): # 自定义的预处理函数\n",
    "    # 调用此函数时会自动传入 x,y 对象，shape 为[b, 28, 28], [b]\n",
    "    # 标准化到 0~1\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [-1, 28*28]) # 打平\n",
    "    y = tf.cast(y, dtype=tf.int32)  # 转成整形张量\n",
    "    y = tf.one_hot(y, depth=10) # one-hot 编码\n",
    "    # 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环训练\n",
    "对于 Dataset 对象，在使用时可以通过  \n",
    "`for step, (x,y) in enumerate(train_db):` 迭代数据集对象，带 step 参数或  \n",
    "`for x,y in train_db:` 迭代数据集对象  \n",
    "我们一般把完成一个 batch 的数据训练，叫做一个 step；通过多个 step 来完成整个训练集的一次迭代，叫做一个 epoch。  \n",
    "也可以通过设置: `train_db = train_db.repeat(20)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
