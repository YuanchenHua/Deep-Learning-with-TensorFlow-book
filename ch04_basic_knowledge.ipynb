{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def printl(a):\n",
    "    print(a)\n",
    "    print('====================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据类型\n",
    "## 数值类型\n",
    "### 标量,单个数字，shape为[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "====================================================\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "====================================================\n",
      "True\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "a = 1.1\n",
    "aa =tf.constant(a)\n",
    "printl(type(a))\n",
    "printl(type(aa))\n",
    "printl(tf.is_tensor(aa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量shape 为[n,]\n",
    "与标量不同，向量的定义须通过 List 类型传给 tf.constant()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.  2.  3.3]], shape=(1, 3), dtype=float32)\n",
      "====================================================\n",
      "tf.Tensor([1.  2.  3.3], shape=(3,), dtype=float32)\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# 注意shape的区别，类似numpy\n",
    "x = tf.constant([[1,2.,3.3]])\n",
    "printl(x)\n",
    "y = tf.constant([1,2.,3.3])\n",
    "printl(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵  shape为[n,m]\n",
    "numpy()方法可以返回 Numpy.array 类型的数据，方便导出数据到系统的其他 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 3.3]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "printl(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3维张量（2纬以上）：  \n",
    "第一维一般理解为\"个数\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]], shape=(3, 2, 2), dtype=int32)\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[[1,2],[3,4]],[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "printl(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符串类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello, Deep Learning.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant('Hello, Deep Learning.')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 tf.strings 模块中，提供了常见的字符串型的工具函数，如拼接 join()，长度 length()，切 分 split()等等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'hello, deep learning.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "b =tf.strings.lower(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boolean 类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 的布尔类型和 Python 语言的布尔类型对等，能通用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(True)\n",
    "if a:\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数值精度\n",
    "对于数值类型的张量，可以保持为不同字节长度的精度，如浮点数 3.14 既可以保存为 16-bit 长度，也可以保存为 32-bit 甚至 64-bit 的精度。Bit 位越长，精度越高，同时占用的 内存空间也就越大。常用的精度类型有 tf.int16, tf.int32, tf.int64, tf.float16, tf.float32, tf.float64，其中 tf.float64 即为 tf.double。  \n",
    "不指定的话，**默认 `tf.float32` 和 `tf.int32`**  \n",
    "对于大部分深度学习算法，一般使用 tf.int32, tf.float32 可满足运算精度要求，部分对 精度要求较高的算法，如**_强化学习_**，可以选择使用 `tf.int64`, `tf.float64` 精度保存张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int16)\n",
      "====================================================\n",
      "tf.Tensor(1.2, shape=(), dtype=float32)\n",
      "====================================================\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1,dtype = tf.int16)\n",
    "printl(a)\n",
    "b =tf.constant(1.2)\n",
    "printl(b)\n",
    "c =tf.constant(1)\n",
    "printl(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度转换\n",
    "使用`tf.cast(data，type)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2 ,dtype = tf.int16)\n",
    "a = tf.cast(a,tf.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行类型转换时，需要保证转换操作的合法性，例如将高精度的张量转换为低精度的张量时，可能发生数据**溢出隐患**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-13035, shape=(), dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(123456789, dtype=tf.int32)\n",
    "\n",
    "print(tf.cast(a, tf.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "布尔型与整形之间相互转换  \n",
    "整数转bool，只要不是0，都算True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0], shape=(2,), dtype=int16)\n",
      "====================================================\n",
      "tf.Tensor([ True  True False  True], shape=(4,), dtype=bool)\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([True, False])\n",
    "printl(tf.cast(a, tf.int16))\n",
    "b = tf.constant([1, 2, 0, 4])\n",
    "printl(tf.cast(b, tf.bool))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 待优化张量\n",
    "为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量，TensorFlow 增加了 一种专门的数据类型来支持梯度信息的记录：`tf.Variable`。`tf.Variable` 类型在普通的张量类 型基础上添加了 `name`，`trainable` 等属性来支持计算图的构建。由于梯度运算会消耗大量的 计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入 `X`， 不需要通过 `tf.Variable` 封装；相反，对于需要计算梯度并优化的张量，如神经网络层的`W` 和`𝒃`，需要通过 `tf.Variable` 包裹以便 TensorFlow 跟踪相关梯度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Variable:0', True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([-1, 0, 1, 2])\n",
    "\n",
    "aa = tf.Variable(a)\n",
    "\n",
    "aa.name, aa.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name 属性用于命名计算图中的变量，这套命名体系是 TensorFlow 内部维护的，一般**不需要用户关注 name 属性**；trainable 表征当前张量是否需要被优化，创建 Variable 对象是默认启用优化标志，可以设置 `trainable=False` 来设置张量不需要优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接创建优化tensor，即 trainable的tensor\n",
    "b = tf.Variable([2,3,4.])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待优化张量可看做普通张量的特殊类型，普通张量也可以通过 `GradientTape.watch()`方法**<font color=red>临时</font>**加入跟踪梯度信息的列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建张量\n",
    "## 从 Numpy, List 对象创建\n",
    "### `tf.convert_to_tensor()` \n",
    "可以创建新 Tensor，并将保存在 Python List 对象或者 Numpy Array 对象中的数据导入到新 Tensor 中：\n",
    "需要注意的是，Numpy中浮点数数组默认使用 **64-Bit** 精度保存数据，转换到 Tensor 类型时精度为 **tf.float64**，可以在需要的时候转换为 tf.float32 类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=133, shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor([1,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=134, shape=(2, 2), dtype=float64, numpy=\n",
       "array([[1., 2.],\n",
       "       [3., 4.]])>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(np.array([[1,2.],[3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建全 0，全 1 张量\n",
    "通过 `tf.zeros()`和 `tf.ones()`即可创建任意形 状全 0 或全 1 的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=155, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.zeros((3,4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=158, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.ones((3,4))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 `tf.zeros_like()`, `tf.ones_like()` 可以方便地新建与某个张量 shape 一致，内容全 0 或全 1 的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=166, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,2,3],[3,4,5]])\n",
    "b =tf.ones_like(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建自定义数值张量\n",
    "`tf.fill(shape, value)` shape必须是个一个向量，不能是矩阵，或者已有的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=189, shape=(3, 4), dtype=int32, numpy=\n",
       "array([[99, 99, 99, 99],\n",
       "       [99, 99, 99, 99],\n",
       "       [99, 99, 99, 99]], dtype=int32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.fill((3,4),99)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建已知分布的张量\n",
    "###  `tf.random.normal(shape, mean=0.0, stddev=1.0)`\n",
    "可以创建形状为 shape，均值为 mean，标准差为 stddev 的正态分布𝒩(𝑚𝑒𝑎𝑛, 𝑠𝑡𝑑𝑑𝑒𝑣 2 )。默认 $mean= 0$ and $stddev = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=201, shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-0.8434031 , -1.3420401 ,  0.44487366, -0.4197785 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=207, shape=(1, 4), dtype=float32, numpy=array([[26.382725 , 19.394056 ,  2.7527008, 16.244581 ]], dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([1,4],20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)`\n",
    "可以创建采样自 [𝑚𝑖𝑛𝑣𝑎𝑙, 𝑚𝑎𝑥𝑣𝑎𝑙]区间的均匀分布的张量。默认$minval = 0$ and $maxval = 1$  \n",
    "如果需要均匀采样整形类型的数据，必须指定采样区间的最大值 maxval 参数，同时制定数 据类型为 `tf.int*`型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=221, shape=(3, 4), dtype=int32, numpy=\n",
       "array([[13, 15, 19, 18],\n",
       "       [18, 18, 13,  6],\n",
       "       [ 9, 16,  9, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([3,4],5,20,dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建序列\n",
    "`tf.range()函数实现。tf.range(limit, delta=1)`可以创建[0, 𝑙𝑖𝑚𝑖𝑡)之间，步长为 delta 的整形序 列，不包含 limit 本身\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=225, shape=(20,), dtype=int32, numpy=\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19], dtype=int32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=262, shape=(7,), dtype=int32, numpy=array([ 0,  3,  6,  9, 12, 15, 18], dtype=int32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(21,delta=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=266, shape=(4,), dtype=int32, numpy=array([10, 13, 16, 19], dtype=int32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10,21,delta=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量的典型应用\n",
    "## 标量\n",
    "`tf.reduce_mean()` 计算平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.5363755e-01 1.6698968e-01 1.0132611e-01 3.2156944e-02 6.9034457e-01\n",
      "  2.9455686e-01 7.8874445e-01 9.7741604e-01 4.6050549e-02 4.0034389e-01]\n",
      " [2.7263474e-01 2.6343882e-01 6.4742112e-01 5.9869289e-03 1.3470149e-01\n",
      "  1.4554381e-01 5.1666629e-01 4.8307872e-01 8.7118387e-01 3.0769384e-01]\n",
      " [4.3996096e-01 7.6022303e-01 3.0946457e-01 4.3136072e-01 3.1962788e-01\n",
      "  9.4238496e-01 8.4017599e-01 1.6197681e-01 8.5438359e-01 9.5582044e-01]\n",
      " [1.6968310e-01 3.1218374e-01 4.2423487e-02 1.8907833e-01 6.5188420e-01\n",
      "  1.7191887e-02 1.9816482e-01 4.8855257e-01 4.9607027e-01 6.7353249e-04]], shape=(4, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 10), dtype=float32)\n",
      "tf.Tensor([0.32040614 0.2944219  0.48003635 0.17737228], shape=(4,), dtype=float32)\n",
      "tf.Tensor(0.31805915, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "out = tf.random.uniform([4,10]) #随机模拟网络输出\n",
    "print(out)\n",
    "y = tf.constant([2,3,2,0]) # 随机构造样本真实标签\n",
    "\n",
    "y = tf.one_hot(y, depth=10) # one-hot 编码\n",
    "print(y)\n",
    "loss = tf.keras.losses.mse(y, out) # 计算每个样本的 MSE\n",
    "print(loss)\n",
    "loss = tf.reduce_mean(loss) # 平均 MSE\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量\n",
    "通过高层接口类 `Dense()`方式创建的网络层，张量 W 和𝒃存储在类的内部，由类自动创 建并管理。可以通过全连接层的 `bias` 成员变量查看偏置变量𝒃，例如创建输入节点数为 4， 输出节点数为 3 的线性层网络，那么它的偏置向量 b 的长度应为 3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = tf.keras.layers.Dense(3) # 创建一层 Wx+b，输出节点为 3\n",
    "\n",
    "# 通过 build 函数创建 W,b 张量，输入节点为 4\n",
    "\n",
    "fc.build(input_shape=(2,4))\n",
    "\n",
    "fc.bias # 查看偏置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       "array([[-0.7828743 ,  0.37297952, -0.14484924],\n",
       "       [ 0.8807198 , -0.41590613, -0.8967582 ],\n",
       "       [ 0.16176462, -0.4971108 , -0.83295697],\n",
       "       [ 0.29150033, -0.31457525,  0.22527957]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = tf.keras.layers.Dense(3) # 定义全连接层的输出节点为 3\n",
    "\n",
    "fc.build(input_shape=(2,4)) # 定义全连接层的输入节点为 4\n",
    "\n",
    "fc.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3维张量\n",
    "$$𝑋 = [𝑏, 𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒 𝑙𝑒𝑛, 𝑓𝑒𝑎𝑡𝑢𝑟𝑒 𝑙𝑒𝑛]$$\n",
    "其中𝑏表示序列信号的数量，sequence len 表示序列信号在时间维度上的采样点数，feature len 表示每个点的特征长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 80)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动加载 IMDB 电影评价数据集\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 将句子填充、截断为等长 80 个单词的句子\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=80)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 x_train 张量的 shape 为[25000,80]，其中 25000 表示句子个数，80 表示每个句子 共 80 个单词，每个单词使用数字编码方式。我们通过 `layers.Embedding()` 层将数字编码的单 词转换为长度为 100 个词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 80, 100])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建词向量 Embedding 层类\n",
    "\n",
    "embedding=tf.keras.layers.Embedding(10000, 100)\n",
    "\n",
    "# 将数字编码的单词转换为词向量\n",
    "\n",
    "out = embedding(x_train)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4维张量\n",
    "4 维张量在卷积神经网络中应用的非常广泛，它用于保存特征图(Feature maps)数据， 格式一般定义为\n",
    "\n",
    "$$[𝑏, ℎ, w, 𝑐]$$\n",
    "其中𝑏表示输入的数量，h/w分布表示特征图的高宽，𝑐 表示特征图的通道数，部分深度学 习框架也会使用[𝑏, 𝑐, ℎ, ]格式的特征图张量，例如 PyTorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 30, 30, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 32x32 的彩色图片输入，个数为 4\n",
    "\n",
    "x = tf.random.normal([4,32,32,3])\n",
    "\n",
    "# 创建卷积神经网络\n",
    "\n",
    "layer = tf.keras.layers.Conv2D(16,kernel_size=3)\n",
    "\n",
    "out = layer(x) # 前向计算\n",
    "\n",
    "out.shape # 输出大小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3, 3, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引与切片\n",
    "索引和numpy一样，可以A[a][b][c] 也可以A[a,b,c]  \n",
    "切片也和numpy一样A[:,3,3] A[...,c]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 维度变换\n",
    "## reshape()\n",
    "为了能够正确恢复出数据，必须保证张量的存储顺序与新视图的维度顺序一致，例如根据图片数量-行-列-通道初始视图保存的张量，按照图片数量-行-列-通道$(𝑏 − ℎ − w − 𝑐)$ 的顺序可以获得合法数据。如果按着图片数量-像素-通道$( b − h ∗ w − c)$的方式恢复视图，也能得到合法的数据。但是如果按着图片数量-通道-像素$(b − c − h ∗ w)$的方式恢复数据， 由于内存布局是按着图片数量-行-列-通道的顺序，视图维度与存储维度顺序相悖，提取的数据将是错乱的。  \n",
    "举个例子，对 于 shape 为$[4,32,32,3]$的图片数据，通过 Reshape 操作将 shape 调整为$[4,1024,3]$，此时视图 的维度顺序为$𝑏 − 𝑝𝑖𝑥𝑒𝑙 − 𝑐$ ，张量的存储顺序为$[𝑏, ℎ, w, 𝑐]。$  \n",
    "通过 `tf.reshape(x, new_shape)`，可以将张量的视图任意的合法改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, TensorShape([2, 4, 4, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(96)\n",
    "x = tf.reshape(x,[2,4,4,3])\n",
    "x.ndim,x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=208, shape=(2, 48), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n",
       "        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
       "       [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
       "        64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
       "        80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(x,[2,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中的参数-1 表示当前轴上长度需要根据视图总元素不变的法则自动推导，从而方便用户书写。比如，上面的-1 可以推导为\n",
    "$$ \\frac{2*4*4*3}{2} = 48$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增删维度\n",
    "### `tf.expand_dims(x, axis)`\n",
    "通过 `tf.expand_dims(x, axis)`可在指定的 **axis 轴前**可以插入一个新的维度  \n",
    "tf.expand_dims 的 axis 为正时，表示在当前维度之前插入一个新维度  \n",
    "为负时，表示**当前维度之后**插入一个新的维度  \n",
    "**其实就是往靠近0的方向插入，无论正负**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform((28,28),5,20)\n",
    "a = tf.expand_dims(a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1就是最后一个纬度，再最后一个纬度后面再插一个，就是最后加一个\n",
    "a = tf.expand_dims(a,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.squeeze(x, axis)`\n",
    "如果希望将 图片数量维度删除，可以通过 `tf.squeeze(x, axis)`函数，`axis` 参数为待删除的维度的索引号  \n",
    "如果不指定维度参数 axis，即` tf.squeeze(x)`，那么他会默认**删除所有**长度为 1 的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.squeeze(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.squeeze(a, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交换维度\n",
    "### `tf.transpose(x, perm) `\n",
    "交换维度(Transpose)。通过交换维度，**改变了张量的存储顺序**，同时也改变了张量的视图。后续的所有操作必须**基于新的存续顺序**进行 。\n",
    "原维度索引分别为 0,1,2,3\n",
    "新纬度索引 0,3,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal([2,32,32,3])\n",
    "a = tf.transpose(a,perm=[0,3,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据复制\n",
    "### `tf.tile(x, multiples)`\n",
    "multiples 分别指定了每个维度上面的复制倍数，对应位置为 1 表明不复制，为 2 表明新长度为原来的长度的 2 倍，即数据复制一份，以此类推。  \n",
    "需要注意的是，tf.tile 会创建一个新的张量来保存复制后的张量，由于复制操作涉及到大量数据的读写 IO 运算，**计算代价相对较高**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([[1 2]], shape=(1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.constant([1,2])\n",
    "print(b)\n",
    "b = tf.expand_dims(b, axis=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [1 2]\n",
      " [1 2]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.tile(b, multiples=[3,1])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "* 轻量级张量复制的手段  \n",
    "* 只有在需要时才会执行实际存储复制操作  \n",
    "**1. 先把纬度全部向右靠齐**  \n",
    "**2. 广播后的纬度右侧部分一定要与原数据相同（原数据中的1可不相等）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=23, shape=(2, 3, 3, 3), dtype=float32, numpy=\n",
       "array([[[[-0.3009113 , -0.47667378, -0.33907276],\n",
       "         [ 0.90679324, -0.47989577,  0.5545086 ],\n",
       "         [-0.3107798 ,  0.6111532 ,  2.9439762 ]],\n",
       "\n",
       "        [[-0.3009113 , -0.47667378, -0.33907276],\n",
       "         [ 0.90679324, -0.47989577,  0.5545086 ],\n",
       "         [-0.3107798 ,  0.6111532 ,  2.9439762 ]],\n",
       "\n",
       "        [[-0.3009113 , -0.47667378, -0.33907276],\n",
       "         [ 0.90679324, -0.47989577,  0.5545086 ],\n",
       "         [-0.3107798 ,  0.6111532 ,  2.9439762 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.3009113 , -0.47667378, -0.33907276],\n",
       "         [ 0.90679324, -0.47989577,  0.5545086 ],\n",
       "         [-0.3107798 ,  0.6111532 ,  2.9439762 ]],\n",
       "\n",
       "        [[-0.3009113 , -0.47667378, -0.33907276],\n",
       "         [ 0.90679324, -0.47989577,  0.5545086 ],\n",
       "         [-0.3107798 ,  0.6111532 ,  2.9439762 ]],\n",
       "\n",
       "        [[-0.3009113 , -0.47667378, -0.33907276],\n",
       "         [ 0.90679324, -0.47989577,  0.5545086 ],\n",
       "         [-0.3107798 ,  0.6111532 ,  2.9439762 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.random.normal([3,3])\n",
    "tf.broadcast_to(A, [2,3,3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行张量运算时，有些运算可以在处理不同 shape 的张量时，会隐式自动调用 Broadcasting 机制，如$+，-，*，/$ 等运算等，将参与运算的张量 **Broadcasting 成一个公共 shape**，再进行相应的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=632, shape=(2, 3, 3, 3), dtype=float32, numpy=\n",
       "array([[[[-0.80892646,  0.91634285, -0.30629864],\n",
       "         [-0.27354348,  1.5865757 , -0.98530245],\n",
       "         [ 1.9690009 , -0.17135239,  0.15626061]],\n",
       "\n",
       "        [[-0.46897167,  1.2562977 ,  0.03365618],\n",
       "         [ 0.7185725 ,  2.5786917 ,  0.00681356],\n",
       "         [ 0.25636768, -1.8839856 , -1.5563726 ]],\n",
       "\n",
       "        [[-2.233618  , -0.50834864, -1.7309902 ],\n",
       "         [ 0.09579784,  1.955917  , -0.61596113],\n",
       "         [-1.4984066 , -3.63876   , -3.311147  ]]],\n",
       "\n",
       "\n",
       "       [[[-0.84298015,  0.8822892 , -0.34035233],\n",
       "         [ 1.3139806 ,  3.1740997 ,  0.6022216 ],\n",
       "         [ 1.0649967 , -1.0753566 , -0.7477436 ]],\n",
       "\n",
       "        [[-1.4116297 ,  0.31363973, -0.9090018 ],\n",
       "         [ 0.08624664,  1.9463658 , -0.62551236],\n",
       "         [ 0.94696456, -1.1933887 , -0.86577576]],\n",
       "\n",
       "        [[-3.6648064 , -1.939537  , -3.1621785 ],\n",
       "         [ 0.83670235,  2.6968215 ,  0.12494341],\n",
       "         [ 1.3181095 , -0.8222438 , -0.49463084]]]], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([2,3,3,1])\n",
    "\n",
    "b = tf.random.normal([3,3])\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学运算\n",
    "基本上和python3里的一样  \n",
    "\n",
    "### 常用的 `tf.square(x)`, `tf.sqrt(x)`,`tf.exp(x)`\n",
    "### 奇葩的`tf.math.log(x)`\n",
    "如果希望计算其他底数的对数，可以根据对数的换底公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=639, shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1.,2.])\n",
    "x = 10**x\n",
    "tf.math.log(x)/tf.math.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.matmul(a, b)`\n",
    "TensorFlow 中的矩阵 相乘可以使用批量方式，也就是张量 a,b 的维度数可以大于 2。当张量 a,b 维度数大于 2 时，TensorFlow 会选择 a,b 的最后两个维度进行矩阵相乘，前面所有的维度都视作 Batch 维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 23, 2)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([4,2,23,32])\n",
    "b = tf.random.normal([4,2,32,2])\n",
    "c = a@b\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵相乘函数支持自动 Broadcasting 机制，但最后两位还是要符合矩阵乘法的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 28, 16)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([4,28,32])\n",
    "\n",
    "b = tf.random.normal([32,16])\n",
    "c = a@b\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
